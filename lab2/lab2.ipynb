{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save this file as studentid1_studentid2_lab2.ipynb**, please check this suffix when you upload your lab, especially when you have multiple copy's in the same folder!\n",
    "(Your student-id is the number shown on your student card.)\n",
    "\n",
    "E.g. if you work with 3 people, the notebook should be named:\n",
    "12301230_3434343_1238938934_lab2.ipynb.\n",
    "\n",
    "**This will be parsed by a regexp, so please double check your filename.**\n",
    "\n",
    "Before you turn this problem in, please make sure everything runs correctly. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Note, that **you are not allowed to use Google Colab**.\n",
    "\n",
    "**Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names and email adresses below.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Yoni Schirris\"\n",
    "NAME2 = \"Tycho Grouwstra\"\n",
    "NAME3 = \"\"\n",
    "EMAIL = \"yschirris@gmail.com\"\n",
    "EMAIL2 = \"tychogrouwstra@gmail.com\"\n",
    "EMAIL3 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f4a038bb9b524e62f6ec198362f51c9b",
     "grade": false,
     "grade_id": "cell-8d856208da5d0763",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lab 2: Classification\n",
    "\n",
    "### Machine Learning 1, November 2018\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in this IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact your teaching assistant.\n",
    "* Please write your answers right below the questions.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Use the provided test cells to check if your answers are correct\n",
    "* **Make sure your output and plots are correct before handing in your assignment with Kernel -> Restart & Run All**\n",
    "\n",
    "* **If possible, all your implementations should be vectorized and rely on loops as little as possible. Therefore for some questions, we give you a maximum number of loops that are necessary for an efficient implementation. This number refers to the loops in this particular function and does not count the ones in functions that are called from the function. You should not go above this number for the maximum number of points.**\n",
    "\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\ba}{\\mathbf{a}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bv}{\\mathbf{v}}$\n",
    "$\\newcommand{\\bq}{\\mathbf{q}}$\n",
    "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
    "$\\newcommand{\\bh}{\\mathbf{h}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bT}{\\mathbf{T}}$\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bV}{\\mathbf{V}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3d9c5a44d13bdc7545f1a15d6dc9c8c",
     "grade": false,
     "grade_id": "cell-422dbc02437671ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "plt.rcParams[\"figure.figsize\"] = [9,5]\n",
    "\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "902185d2dda7e356189a57a09a637182",
     "grade": false,
     "grade_id": "cell-7f215df0e22ae748",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell makes sure that you have all the necessary libraries installed\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from importlib.util import find_spec, module_from_spec\n",
    "\n",
    "def check_newer_version(version_inst, version_nec):\n",
    "    version_inst_split = version_inst.split('.')\n",
    "    version_nec_split = version_nec.split('.')\n",
    "    for i in range(min(len(version_inst_split), len(version_nec_split))):\n",
    "        if int(version_nec_split[i]) > int(version_inst_split[i]):\n",
    "            return False\n",
    "        elif int(version_nec_split[i]) < int(version_inst_split[i]):\n",
    "            return True\n",
    "    return True\n",
    "        \n",
    "    \n",
    "module_list = [('jupyter', '1.0.0'), \n",
    "               ('matplotlib', '2.0.2'), \n",
    "               ('numpy', '1.13.1'), \n",
    "               ('python', '3.6.2'), \n",
    "               ('sklearn', '0.19.0'), \n",
    "               ('scipy', '0.19.1'), \n",
    "               ('nb_conda', '2.2.1')]\n",
    "\n",
    "packages_correct = True\n",
    "packages_errors = []\n",
    "\n",
    "for module_name, version in module_list:\n",
    "    if module_name == 'scikit-learn':\n",
    "        module_name = 'sklearn'\n",
    "    if module_name == 'pyyaml':\n",
    "        module_name = 'yaml'\n",
    "    if 'python' in module_name:\n",
    "        python_version = platform.python_version()\n",
    "        if not check_newer_version(python_version, version):\n",
    "            packages_correct = False\n",
    "            error = f'Update {module_name} to version {version}. Current version is {python_version}.'\n",
    "            packages_errors.append(error) \n",
    "            print(error)\n",
    "    else:\n",
    "        spec = find_spec(module_name)\n",
    "        if spec is None:\n",
    "            packages_correct = False\n",
    "            error = f'Install {module_name} with version {version} or newer, it is required for this assignment!'\n",
    "            packages_errors.append(error) \n",
    "            print(error)\n",
    "        else:\n",
    "            x =__import__(module_name)\n",
    "            if hasattr(x, '__version__') and not check_newer_version(x.__version__, version):\n",
    "                packages_correct = False\n",
    "                error = f'Update {module_name} to version {version}. Current version is {x.__version__}.'\n",
    "                packages_errors.append(error) \n",
    "                print(error)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    packages_correct = False\n",
    "    error = \"\"\"Please, don't use google colab!\n",
    "It will make it much more complicated for us to check your homework as it merges all the cells into one.\"\"\"\n",
    "    packages_errors.append(error) \n",
    "    print(error)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "packages_errors = '\\n'.join(packages_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "17f77a32492fcf6ac989eab8a50e4dab",
     "grade": false,
     "grade_id": "cell-821f67d8cd14e4f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1. Multiclass logistic regression\n",
    "\n",
    "Scenario: you have a friend with one big problem: she's completely blind. You decided to help her: she has a special smartphone for blind people, and you are going to develop a mobile phone app that can do _machine vision_ using the mobile camera: converting a picture (from the camera) to the meaning of the image. You decide to start with an app that can read handwritten digits, i.e. convert an image of handwritten digits to text (e.g. it would enable her to read precious handwritten phone numbers).\n",
    "\n",
    "A key building block for such an app would be a function `predict_digit(x)` that returns the digit class of an image patch $\\bx$. Since hand-coding this function is highly non-trivial, you decide to solve this problem using machine learning, such that the internal parameters of this function are automatically learned using machine learning techniques.\n",
    "\n",
    "The dataset you're going to use for this is the MNIST handwritten digits dataset (`http://yann.lecun.com/exdb/mnist/`). You can download the data with scikit learn, and load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d31db37aebf93c82a9408cfa97b788d3",
     "grade": false,
     "grade_id": "cell-bcdbc957165abae7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "import os\n",
    "# Fetch the data\n",
    "try:\n",
    "    mnist = fetch_mldata('MNIST original', data_home='.')\n",
    "except Exception:\n",
    "    raise FileNotFoundError('Please download mnist-original.mat from Canvas and put it in %s/mldata' % os.getcwd())\n",
    "data, target = mnist.data, mnist.target.astype('int')\n",
    "# Shuffle\n",
    "indices = np.arange(len(data))\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(indices)\n",
    "data, target = data[indices].astype('float32'), target[indices]\n",
    "\n",
    "# Normalize the data between 0.0 and 1.0:\n",
    "data /= 255. \n",
    "\n",
    "# Split\n",
    "x_train, x_valid, x_test = data[:50000], data[50000:60000], data[60000: 70000]\n",
    "t_train, t_valid, t_test = target[:50000], target[50000:60000], target[60000: 70000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b20138af0810741223d2c2ddc82bf0f",
     "grade": false,
     "grade_id": "cell-b7b4a5a96dccf229",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "MNIST consists of small 28 by 28 pixel images of written digits (0-9). We split the dataset into a training, validation and testing arrays. The variables `x_train`, `x_valid` and `x_test` are $N \\times M$ matrices, where $N$ is the number of datapoints in the respective set, and $M = 28^2 = 784$ is the dimensionality of the data. The second set of variables `t_train`, `t_valid` and `t_test` contain the corresponding $N$-dimensional vector of integers, containing the true class labels.\n",
    "\n",
    "Here's a visualisation of the first 8 digits of the trainingset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40e1628ec56b6d664edf9aaf496ea637",
     "grade": false,
     "grade_id": "cell-48a92c0a2a2bf4dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107268a58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_digits(data, num_cols, targets=None, shape=(28,28)):\n",
    "    num_digits = data.shape[0]\n",
    "    num_rows = int(num_digits/num_cols)\n",
    "    for i in range(num_digits):\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt.imshow(data[i].reshape(shape), interpolation='none', cmap='Greys')\n",
    "        if targets is not None:\n",
    "            plt.title(int(targets[i]))\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_digits(x_train[0:40000:5000], num_cols=4, targets=t_train[0:40000:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c7e0668b7e246603c3c12d9183c9a83",
     "grade": false,
     "grade_id": "cell-3eb664a58e03bf42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In _multiclass_ logistic regression, the conditional probability of class label $j$ given the image $\\bx$ for some datapoint is given by:\n",
    "\n",
    "$ \\log p(t = j \\;|\\; \\bx, \\bb, \\bW) = \\log q_j - \\log Z$\n",
    "\n",
    "where $\\log q_j = \\bw_j^T \\bx + b_j$ (the log of the unnormalized probability of the class $j$), and $Z = \\sum_k q_k$ is the normalizing factor. $\\bw_j$ is the $j$-th column of $\\bW$ (a matrix of size $784 \\times 10$) corresponding to the class label, $b_j$ is the $j$-th element of $\\bb$.\n",
    "\n",
    "Given an input image, the multiclass logistic regression model first computes the intermediate vector $\\log \\bq$ (of size $10 \\times 1$), using $\\log q_j = \\bw_j^T \\bx + b_j$, containing the unnormalized log-probabilities per class. \n",
    "\n",
    "The unnormalized probabilities are then normalized by $Z$ such that $\\sum_j p_j = \\sum_j \\exp(\\log p_j) = 1$. This is done by $\\log p_j = \\log q_j - \\log Z$ where $Z = \\sum_i \\exp(\\log q_i)$. This is known as the _softmax_ transformation, and is also used as a last layer of many classifcation neural network models, to ensure that the output of the network is a normalized distribution, regardless of the values of second-to-last layer ($\\log \\bq$)\n",
    "\n",
    "**Warning**: when computing $\\log Z$, you are likely to encounter numerical problems. Save yourself countless hours of debugging and learn the [log-sum-exp trick](https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/ \"Title\").\n",
    "\n",
    "The network's output $\\log \\bp$ of size $10 \\times 1$ then contains the conditional log-probabilities $\\log p(t = j \\;|\\; \\bx, \\bb, \\bW)$ for each digit class $j$. In summary, the computations are done in this order:\n",
    "\n",
    "$\\bx \\rightarrow \\log \\bq \\rightarrow Z \\rightarrow \\log \\bp$\n",
    "\n",
    "Given some dataset with $N$ independent, identically distributed datapoints, the log-likelihood is given by:\n",
    "\n",
    "$ \\mathcal{L}(\\bb, \\bW) = \\sum_{n=1}^N \\mathcal{L}^{(n)}$\n",
    "\n",
    "where we use $\\mathcal{L}^{(n)}$ to denote the partial log-likelihood evaluated over a single datapoint. It is important to see that the log-probability of the class label $t^{(n)}$ given the image, is given by the $t^{(n)}$-th element of the network's output $\\log \\bp$, denoted by $\\log p_{t^{(n)}}$:\n",
    "\n",
    "$\\mathcal{L}^{(n)} = \\log p(t = t^{(n)} \\;|\\; \\bx = \\bx^{(n)}, \\bb, \\bW) = \\log p_{t^{(n)}} = \\log q_{t^{(n)}} - \\log Z^{(n)}$\n",
    "\n",
    "where $\\bx^{(n)}$ and $t^{(n)}$ are the input (image) and class label (integer) of the $n$-th datapoint, and $Z^{(n)}$ is the normalizing constant for the distribution over $t^{(n)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "88e0996cf7e7e29fd3886b9002845c98",
     "grade": false,
     "grade_id": "cell-17766ee789f11384",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1.1 Gradient-based stochastic optimization\n",
    "### 1.1.1 Derive gradient equations (20 points)\n",
    "\n",
    "Derive the equations for computing the (first) partial derivatives of the log-likelihood w.r.t. all the parameters, evaluated at a _single_ datapoint $n$.\n",
    "\n",
    "You should start deriving the equations for $\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}$ for each $j$. For clarity, we'll use the shorthand $\\delta^q_j = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}$.\n",
    "\n",
    "For $j = t^{(n)}$:\n",
    "$$\n",
    "\\delta^q_j\n",
    "= \\frac{\\partial \\log q_{t^{(n)}}}{\\partial \\log q_j}\n",
    "-\n",
    "\\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \n",
    "= 1\n",
    "-\n",
    "\\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \n",
    "$$\n",
    "\n",
    "For $j \\neq t^{(n)}$:\n",
    "$$\n",
    "\\delta^q_j\n",
    "= \\frac{\\partial \\log q_{t^{(n)}}}{\\partial \\log q_j}\n",
    "-\n",
    "\\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j} \n",
    "=0 - \\frac{\\partial \\log Z}{\\partial Z} \n",
    "\\frac{\\partial Z}{\\partial \\log q_j}\n",
    "$$\n",
    "\n",
    "Complete the above derivations for $\\delta^q_j$ by furtherly developing $\\frac{\\partial \\log Z}{\\partial Z}$ and $\\frac{\\partial Z}{\\partial \\log q_j}$. Both are quite simple. For these it doesn't matter whether $j = t^{(n)}$ or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f394ea0423ed2b17c80bbe8f1193cc81",
     "grade": true,
     "grade_id": "cell-e40110444a1e1d3f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "For $j = t^{(n)}$:\n",
    "\\begin{align}\n",
    "\\delta^q_j\n",
    "&= 1 - \\frac{q_j}{\\sum_k q_k} = 1 - \\frac{q_j}{Z}\n",
    "\\end{align}\n",
    "For $j \\neq t^{(n)}$:\n",
    "\\begin{align}\n",
    "\\delta^q_j\n",
    "&= 0 - \\frac{q_j}{\\sum_k q_k} = 0 - \\frac{q_j}{Z}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d46c028e9830445397d7b2275815990d",
     "grade": false,
     "grade_id": "cell-c770cfe1389ca4ff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Given your equations for computing the gradients $\\delta^q_j$ it should be quite straightforward to derive the equations for the gradients of the parameters of the model, $\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}}$ and $\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial b_j}$. The gradients for the biases $\\bb$ are given by:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial b_j}\n",
    "= \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial b_j}\n",
    "= \\delta^q_j\n",
    "\\cdot 1\n",
    "= \\delta^q_j\n",
    "$\n",
    "\n",
    "The equation above gives the derivative of $\\mathcal{L}^{(n)}$ w.r.t. a single element of $\\bb$, so the vector $\\nabla_\\bb \\mathcal{L}^{(n)}$ with all derivatives of $\\mathcal{L}^{(n)}$ w.r.t. the bias parameters $\\bb$ is: \n",
    "\n",
    "$\n",
    "\\nabla_\\bb \\mathcal{L}^{(n)} = \\mathbf{\\delta}^q\n",
    "$\n",
    "\n",
    "where $\\mathbf{\\delta}^q$ denotes the vector of size $10 \\times 1$ with elements $\\mathbf{\\delta}_j^q$.\n",
    "\n",
    "The (not fully developed) equation for computing the derivative of $\\mathcal{L}^{(n)}$ w.r.t. a single element $W_{ij}$ of $\\bW$ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "$\n",
    "\n",
    "What is $\\frac{\\partial \\log q_j}{\\partial W_{ij}}$? Complete the equation above.\n",
    "\n",
    "If you want, you can give the resulting equation in vector format ($\\nabla_{\\bw_j} \\mathcal{L}^{(n)} = ...$), like we did for $\\nabla_\\bb \\mathcal{L}^{(n)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "70fc98e5b227736e2bb92964a3c7174d",
     "grade": true,
     "grade_id": "cell-e40110444a1asdfasdfd3f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$\\frac{\\partial log q_j}{\\partial W_{ij}} = \\frac{\\partial (\\textbf{w}_j^T\\textbf{x} + b_j)}{\\partial W_{ij}} = x_i$\n",
    "\n",
    "$\\frac{\\partial log q_j}{\\partial \\textbf{w}_j} = \\frac{\\partial (\\textbf{w}_j^T\\textbf{x} + b_j)}{\\partial \\textbf{w}_j} = \\textbf{x}^T$\n",
    "\n",
    "so\n",
    "\n",
    "for $j = t^{(n)}$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\delta_j^q x_i\n",
    "= x_i (1 - \\frac{q_j}{\\sum_k q_k})\n",
    "= x_i (1 - \\frac{q_j}{Z})\n",
    "$ \n",
    " \n",
    "for $j \\neq t^{(n)}$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\delta_j^q x_i\n",
    "= - x_i q_j  \\cdot \\sum_k \\frac{1}{q_k}\n",
    "= -x_i \\frac{q_j}{\\sum_k q_k}\n",
    "= -x_i \\frac{q_j}{Z}\n",
    "$\n",
    "\n",
    "In vector notation, using the required delta:\n",
    "\n",
    "$\n",
    "\\nabla_{w_j} \\mathcal{L}^{(n)} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial \\textbf{w}_j}\n",
    "= \\mathbf{\\delta}^q\n",
    "\\frac{\\partial \\log q_j}{\\partial \\textbf{w}_j}\n",
    "= \\delta^q \\textbf{x}^T  \n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7744e9051681182c7e1af0c515622fce",
     "grade": false,
     "grade_id": "cell-b0f28b0924b9983d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1.2 Implement gradient computations (15 points)\n",
    "\n",
    "Implement the gradient calculations you derived in the previous question. Write a function `logreg_gradient(x, t, w, b)` that returns the gradients $\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ (for each $j$) and $\\nabla_{\\bb} \\mathcal{L}^{(n)}$, i.e. the first partial derivatives of the log-likelihood w.r.t. the parameters $\\bW$ and $\\bb$, evaluated at a single datapoint (`x`, `t`).\n",
    "The computation will contain roughly the following intermediate variables:\n",
    "\n",
    "$\n",
    "\\log \\bq \\rightarrow Z \\rightarrow \\log \\bp\\,,\\, \\mathbf{\\delta}^q\n",
    "$\n",
    "\n",
    "followed by computation of the gradient vectors $\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ (contained in a $784 \\times 10$ matrix) and $\\nabla_{\\bb} \\mathcal{L}^{(n)}$ (a $10 \\times 1$ vector).\n",
    "\n",
    "For maximum points, ensure the function is numerically stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0d41cbf33ddb93414c789220c024c45",
     "grade": false,
     "grade_id": "cell-6858f885be587480",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.1.2 Compute gradient of log p(t|x;w,b) wrt w and b\n",
    "from scipy.special import logsumexp \n",
    "\n",
    "def logreg_gradient(x, t, w, b):\n",
    "    logq = x.dot(w) + b\n",
    "    q = np.exp(logq)\n",
    "    logZ = logsumexp(logq)\n",
    "    Z = np.exp(logZ)\n",
    "    logp = logq - logZ\n",
    "    delta_q = -1/Z * q\n",
    "    dL_db = delta_q\n",
    "    dL_db[:,t[0]] += 1\n",
    "    dL_dw = np.outer(x, dL_db)\n",
    "    return logp[:,t].squeeze(), dL_dw, dL_db.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a86683efa9cf9356b51f08cb6d2599b7",
     "grade": true,
     "grade_id": "cell-48057487182fe951",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden tests for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "281f0c66a2c9bfdf53c1995a852eea1f",
     "grade": true,
     "grade_id": "cell-1c9659f607b151a2",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient on one point\n",
      "Log Likelihood:\t -2.2959726720744777\n",
      "\n",
      "Grad_W_ij\t (784, 10) matrix\n",
      "Grad_W_ij[0,152:158]=\t [-0.04518971 -0.06758809 -0.07819784 -0.09077237 -0.07584012 -0.06365855]\n",
      "\n",
      "Grad_B_i shape\t (10,) vector\n",
      "Grad_B_i=\t [-0.10020327 -0.09977827 -0.1003198   0.89933657 -0.10037941 -0.10072863\n",
      " -0.09982729 -0.09928672 -0.09949324 -0.09931994]\n",
      "i in {0,...,9}; j in M\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "# scalar, 10 X 768  matrix, 10 X 1 vector\n",
    "w = np.random.normal(size=(28*28,10), scale=0.001)\n",
    "# w = np.zeros((784,10))\n",
    "b = np.zeros((10,))\n",
    "\n",
    "# test gradients, train on 1 sample\n",
    "logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w, b)\n",
    "\n",
    "print(\"Test gradient on one point\")\n",
    "print(\"Log Likelihood:\\t\", logpt)\n",
    "print(\"\\nGrad_W_ij\\t\",grad_w.shape,\"matrix\")\n",
    "print(\"Grad_W_ij[0,152:158]=\\t\", grad_w[152:158,0])\n",
    "print(\"\\nGrad_B_i shape\\t\",grad_b.shape,\"vector\")\n",
    "print(\"Grad_B_i=\\t\", grad_b.T)\n",
    "print(\"i in {0,...,9}; j in M\")\n",
    "\n",
    "assert logpt.shape == (), logpt.shape\n",
    "assert grad_w.shape == (784, 10), grad_w.shape\n",
    "assert grad_b.shape == (10,), grad_b.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ec5c9e0bdfc89e9d8e34971863dd612",
     "grade": true,
     "grade_id": "cell-fd59c3a03a87ab83",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite difference error grad_w: 6.36129468997e-07\n",
      "Finite difference error grad_b: 5.23511748726e-08\n"
     ]
    }
   ],
   "source": [
    "# It's always good to check your gradient implementations with finite difference checking:\n",
    "# Scipy provides the check_grad function, which requires flat input variables.\n",
    "# So we write two helper functions that provide the gradient and output with 'flat' weights:\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "np.random.seed(123)\n",
    "# scalar, 10 X 768  matrix, 10 X 1 vector\n",
    "w = np.random.normal(size=(28*28,10), scale=0.001)\n",
    "# w = np.zeros((784,10))\n",
    "b = np.zeros((10,))\n",
    "\n",
    "def func(w):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w.reshape(784,10), b)\n",
    "    return logpt\n",
    "def grad(w):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w.reshape(784,10), b)\n",
    "    return grad_w.flatten()\n",
    "finite_diff_error = check_grad(func, grad, w.flatten())\n",
    "print('Finite difference error grad_w:', finite_diff_error)\n",
    "assert finite_diff_error < 1e-3, 'Your gradient computation for w seems off'\n",
    "\n",
    "def func(b):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w, b)\n",
    "    return logpt\n",
    "def grad(b):\n",
    "    logpt, grad_w, grad_b = logreg_gradient(x_train[0:1,:], t_train[0:1], w, b)\n",
    "    return grad_b.flatten()\n",
    "finite_diff_error = check_grad(func, grad, b)\n",
    "print('Finite difference error grad_b:', finite_diff_error)\n",
    "assert finite_diff_error < 1e-3, 'Your gradient computation for b seems off'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4bb956f91b61cedbca19fe84c4b7fd44",
     "grade": true,
     "grade_id": "cell-91b8c5eb86f6a0f3",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL!\n",
    "# It contains hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f2bf7605bef859967bd38bb1c3c384c",
     "grade": false,
     "grade_id": "cell-bdce061b39aaacec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### 1.1.3 Stochastic gradient descent (15 points)\n",
    "\n",
    "Write a function `sgd_iter(x_train, t_train, w, b)` that performs one iteration of stochastic gradient descent (SGD), and returns the new weights. It should go through the trainingset once in randomized order, call `logreg_gradient(x, t, w, b)` for each datapoint to get the gradients, and update the parameters **using a small learning rate of `1e-6`**. Note that in this case we're maximizing the likelihood function, so we should actually performing gradient ___ascent___... For more information about SGD, see Bishop 5.2.4 or an online source (i.e. https://en.wikipedia.org/wiki/Stochastic_gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a1fd10093bd350a24e4a718bc0133738",
     "grade": true,
     "grade_id": "cell-86bf84658f1c5bc8",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sgd_iter(x_train, t_train, W, b, val=False):\n",
    "    lr = 10**-4\n",
    "    arr = np.arange(0,len(x_train))\n",
    "    np.random.shuffle(arr)\n",
    "    logp_tracker = 0\n",
    "\n",
    "    for i in arr:\n",
    "      logp_train, grad_w, grad_b = logreg_gradient(x_train[i:i+1, :], t_train[i:i+1], W.reshape(784,10), b)\n",
    "      logp_tracker += logp_train\n",
    "      if not val:\n",
    "        W = W + lr * grad_w\n",
    "        b = b + lr * grad_b\n",
    "    return logp_tracker/len(arr), W, b\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f9dc9179ce6db8ed7eba9ea02ed82723",
     "grade": true,
     "grade_id": "cell-0929d502114babdb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden tests for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e5a417580d51fc1c3ef7519047eead4",
     "grade": true,
     "grade_id": "cell-2f7bbc264cc887a0",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "np.random.seed(1243)\n",
    "w = np.zeros((28*28, 10))\n",
    "b = np.zeros(10)\n",
    "    \n",
    "logp_train, W, b = sgd_iter(x_train[:5], t_train[:5], w, b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a60e2b48d4b187276ec9e8f8ef86a5f3",
     "grade": false,
     "grade_id": "cell-81634c804e1f93fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1.2. Train\n",
    "\n",
    "### 1.2.1 Train (12 points)\n",
    "Perform SGD on the training set. Plot (in one graph) the conditional log-probability of the training set and validation set after each iteration. (6 points)\n",
    "\n",
    "Instead of running SGD for a fixed number of steps, run it until convergence. Think of a reasonable criterion for determining convergence. As a reference: choose a criterion such that the algorithm terminates in less than 15 iterations over the training set. (2 points)\n",
    "\n",
    "Make sure your implementation (in particular, the output of the conditional log-probability of the training set and validation set) is independent of the size of the dataset. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "221a9af93fcc6e5ab77fc26652c80a5f",
     "grade": true,
     "grade_id": "cell-20a347ba4db6e82c",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10727a0b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_sgd(x_train, t_train, x_valid, t_valid, w, b):\n",
    "    x=[]\n",
    "    x_counter=0\n",
    "    y_train=[]\n",
    "    y_val=[]\n",
    "    converged=False\n",
    "    previous_logp = None\n",
    "    likelihood_growth_criterion = -0.05\n",
    "    \n",
    "    while not converged:\n",
    "      logp_train, w, b = sgd_iter(x_train, t_train, w, b)\n",
    "      logp_val, w_val, b_val = sgd_iter(x_valid, t_valid, w, b, val=True)\n",
    "      if previous_logp:\n",
    "        if ((logp_val - previous_logp) / previous_logp) >= likelihood_growth_criterion:\n",
    "          converged = True\n",
    "          \n",
    "      previous_logp = logp_val\n",
    "      y_train.append(logp_train)\n",
    "      y_val.append(logp_val)\n",
    "\n",
    "      x.append(x_counter)\n",
    "\n",
    "      x_counter += 1\n",
    "    plt.plot(x, y_train, 'b-o', linewidth=0.4, label='Running average of conditional logp of training set')\n",
    "    plt.plot(x, y_val, 'b--o', label='Conditional logp of optimized weights on the validation set')\n",
    "    plt.title('Plot of the logprobabilities of the training and validation data set after each epoch.\\nThe training set in blue gradually updates its weights over an epoch and is then averaged\\nThe validation set in green uses the optimal weights after an entire epoch of weight updates.\\n It is said to be converged when the conditional log probability of the validation dataset decreases by less than 5% over an epoch')\n",
    "    plt.xlabel(\"Numbers of Epochs\")\n",
    "    plt.ylabel(\"Conditional Log Probability\")\n",
    "    plt.legend()\n",
    "    return(w, b)\n",
    "    \n",
    "np.random.seed(1243)\n",
    "w = np.random.normal(size=(28*28,10), scale=0.001)\n",
    "b = np.zeros(10)\n",
    "w,b = test_sgd(x_train, t_train, x_valid, t_valid, w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d46a6466493fa88818a210decebf40a",
     "grade": true,
     "grade_id": "cell-b290fe89d0aa4ffb",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden tests for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a6d85bbd97cad35d524b65b23f64e75f",
     "grade": false,
     "grade_id": "cell-cf7f3da57d19493a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2.2 Visualize weights (10 points)\n",
    "Visualize the resulting parameters $\\bW$ after a few iterations through the training set, by treating each column of $\\bW$ as an image. If you want, you can use or edit the `plot_digits(...)` above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4e554436500eebe1527a31039570a264",
     "grade": true,
     "grade_id": "cell-b10656f35fac065e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #Need to rerun these 2 lines, as otherwise the plots are too small (corrupted with size)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline #Need to rerun these 2 lines, as otherwise the plots are too small (corrupted with size)\n",
    "plt.rcParams[\"figure.figsize\"] = [9,5]\n",
    "\n",
    "def plot_digits_2(data, num_cols=5, targets=None, shape=(28,28)):\n",
    "    num_digits = data.shape[0]\n",
    "    num_rows = int(num_digits/num_cols)\n",
    "    for i in range(num_digits):\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt.imshow(data[i].reshape(shape), interpolation='none', cmap='jet')\n",
    "        if targets is not None:\n",
    "            plt.title(int(targets[i]))\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_digits_2(w.T, targets=[0,1,2,3,4,5,6,7,8,9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de187afcf5ae1e34b80bc10610760e7a",
     "grade": true,
     "grade_id": "cell-eb131c8b7303da38",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Describe in less than 100 words why these weights minimize the loss**\n",
    "\n",
    "When data is encountered in a 'hot' area, the probability for that number is increased accordingly. Therefore, if there's a written number where many of its pixels fall in the area that's defined in one of the hot areas of the above 'boxes', the probability is continuously increased for each pixel that falls in one of the hot areas.\n",
    "\n",
    "Also: There are negative weights. Pixels in these areas indicate that it is more likely that the input belongs to another true class, and therefore assigns a negative weights for that specific class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a6a97ce145be9d58d5cf190e49da491",
     "grade": false,
     "grade_id": "cell-f36d974d9ef34c97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2.3. Visualize the 8 hardest and 8 easiest digits (10 points)\n",
    "Visualize the 8 digits in the validation set with the highest probability of the true class label under the model.\n",
    "Also plot the 8 digits that were assigned the lowest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4f212e606b9d9d7fd4ae403d643dacfd",
     "grade": true,
     "grade_id": "cell-3802d61680deeff5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# get the logp for each datapoint and its actual class\n",
    "listofposteriors = [\n",
    "    (idx, logreg_gradient(x_valid[idx:idx+1, :], t_valid[idx:idx+1], w.reshape(784,10), b)[0])\n",
    "    for idx, x in enumerate(x_valid)\n",
    "]\n",
    "# sort the logps\n",
    "listofposteriors.sort(key=lambda tup: tup[1])\n",
    "\n",
    "high_prob_data = []\n",
    "high_prob_targets = []\n",
    "\n",
    "\n",
    "# i get the 8 at the top, the highest probs\n",
    "for tpl in listofposteriors[:-9:-1]:\n",
    "  high_prob_data.append(x_valid[tpl[0]])\n",
    "  high_prob_targets.append(t_valid[tpl[0]])\n",
    "\n",
    "\n",
    "# and the 8 at the bottom, the least probable\n",
    "  \n",
    "low_prob_data = []\n",
    "low_prob_targets = []\n",
    " \n",
    "for tpl in listofposteriors[:8]:\n",
    "  low_prob_data.append(x_valid[tpl[0]])\n",
    "  low_prob_targets.append(t_valid[tpl[0]])\n",
    "  \n",
    "high_prob_data = np.array(high_prob_data)\n",
    "low_prob_data = np.array(low_prob_data)\n",
    "\n",
    "# then i plot those data points\n",
    "print(\"Digits with highest probability of their true class\")\n",
    "plot_digits(np.array(high_prob_data), num_cols=4, targets=high_prob_targets)\n",
    "print(\"Digits with lowest probability of their true class\")\n",
    "plot_digits(np.array(low_prob_data), num_cols=4, targets=low_prob_targets)\n",
    "\n",
    "\n",
    "#print(listofposteriors[:-10: -1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "642d43cd6542e6cf49779799becbd435",
     "grade": true,
     "grade_id": "cell-6564a51fdda06d95",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Ask yourself if these results make sense. Explain in no more then two sentences what it means that a digit is hard to classify.\n",
    "\n",
    "These results seem to make sense -- the digits classified with high confidence admittedly look quite clear, while the ones classified with low confidence will generally look less neat to a human reader as well. A digit is hard to classify when its input badly corresponds to our learned weights (= hot areas in above heatmap) for its correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21418c6f8f5a8b5bd54c2b6fd655ec90",
     "grade": false,
     "grade_id": "cell-2c525344c99e5b26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2. Multilayer perceptron\n",
    "\n",
    "\n",
    "You discover that the predictions by the logistic regression classifier are not good enough for your application: the model is too simple. You want to increase the accuracy of your predictions by using a better model. For this purpose, you're going to use a multilayer perceptron (MLP), a simple kind of neural network. The perceptron will have a single hidden layer $\\bh$ with $L$ elements. The parameters of the model are $\\bV$ (connections between input $\\bx$ and hidden layer $\\bh$), $\\ba$ (the biases/intercepts of $\\bh$), $\\bW$ (connections between $\\bh$ and $\\log q$) and $\\bb$ (the biases/intercepts of $\\log q$).\n",
    "\n",
    "The conditional probability of the class label $j$ is given by:\n",
    "\n",
    "$\\log p(t = j \\;|\\; \\bx, \\bb, \\bW) = \\log q_j - \\log Z$\n",
    "\n",
    "where $q_j$ are again the unnormalized probabilities per class, and $Z = \\sum_j q_j$ is again the probability normalizing factor. Each $q_j$ is computed using:\n",
    "\n",
    "$\\log q_j = \\bw_j^T \\bh + b_j$\n",
    "\n",
    "where $\\bh$ is a $L \\times 1$ vector with the hidden layer activations (of a hidden layer with size $L$), and $\\bw_j$ is the $j$-th column of $\\bW$ (a $L \\times 10$ matrix). Each element of the hidden layer is computed from the input vector $\\bx$ using:\n",
    "\n",
    "$h_j = \\sigma(\\bv_j^T \\bx + a_j)$\n",
    "\n",
    "where $\\bv_j$ is the $j$-th column of $\\bV$ (a $784 \\times L$ matrix), $a_j$ is the $j$-th element of $\\ba$, and $\\sigma(.)$ is the so-called sigmoid activation function, defined by:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "Note that this model is almost equal to the multiclass logistic regression model, but with an extra 'hidden layer' $\\bh$. The activations of this hidden layer can be viewed as features computed from the input, where the feature transformation ($\\bV$ and $\\ba$) is learned.\n",
    "\n",
    "## 2.1 Derive gradient equations (20 points)\n",
    "\n",
    "State (shortly) why $\\nabla_{\\bb} \\mathcal{L}^{(n)}$ is equal to the earlier (multiclass logistic regression) case, and why $\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ is almost equal to the earlier case.\n",
    "\n",
    "Like in multiclass logistic regression, you should use intermediate variables $\\mathbf{\\delta}_j^q$. In addition, you should use intermediate variables $\\mathbf{\\delta}_j^h = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial h_j}$.\n",
    "\n",
    "Given an input image, roughly the following intermediate variables should be computed:\n",
    "\n",
    "$\n",
    "\\log \\bq \\rightarrow Z \\rightarrow \\log \\bp \\rightarrow \\mathbf{\\delta}^q \\rightarrow \\mathbf{\\delta}^h\n",
    "$\n",
    "\n",
    "where $\\mathbf{\\delta}_j^h = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\bh_j}$.\n",
    "\n",
    "Give the equations for computing $\\mathbf{\\delta}^h$, and for computing the derivatives of $\\mathcal{L}^{(n)}$ w.r.t. $\\bW$, $\\bb$, $\\bV$ and $\\ba$. \n",
    "\n",
    "You can use the convenient fact that $\\frac{\\partial}{\\partial x} \\sigma(x) = \\sigma(x) (1 - \\sigma(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bb7ce29f01484f94a6357784ddaf6412",
     "grade": true,
     "grade_id": "cell-48f48bb8ec75cc3c",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "### State (shortly) why $\\nabla_{\\bb} \\mathcal{L}^{(n)}$ is equal to the earlier (multiclass logistic regression) case, and why $\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ is almost equal to the earlier case.\n",
    "\n",
    "$\\nabla_{\\bb} \\mathcal{L}^{(n)}$ is equal to the earlier (multiclass logistic regression) case because it does not depend on anything going on in the new hidden layer (or anything from before for that matter).\n",
    "\n",
    "$\\nabla_{\\bw_j} \\mathcal{L}^{(n)}$ is almost equal to the earlier case, but now it depends on $h$ for its input instead of directly using $x$.\n",
    "\n",
    "### Give the equations for computing $\\mathbf{\\delta}^h$, and for computing the derivatives of $\\mathcal{L}^{(n)}$ w.r.t. $\\bW$, $\\bb$, $\\bV$ and $\\ba$. \n",
    "\n",
    "<!-- \n",
    "--------------------\n",
    "\n",
    "\n",
    "$\\frac{\\partial log q_j}{\\partial W_{ij}} = \\frac{\\partial (\\textbf{w}_j^T\\textbf{x} + b_j)}{\\partial W_{ij}} = x_i$\n",
    "\n",
    "$\\frac{\\partial log q_j}{\\partial \\textbf{w}_j} = \\frac{\\partial (\\textbf{w}_j^T\\textbf{x} + b_j)}{\\partial \\textbf{w}_j} = \\textbf{x}^T$\n",
    "\n",
    "so\n",
    "\n",
    "for $j = t^{(n)}$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\delta_j^q x_i\n",
    "= x_i - x_i q_j  \\cdot \\sum_k \\frac{1}{q_k}\n",
    "$\n",
    "\n",
    "as matrix it is..\n",
    "\n",
    "$\n",
    "\\textbf{x}^T \\delta^q\n",
    "$\n",
    "\n",
    "1x784 x 10x1 = 784*10 matrix, so this might be correct\n",
    "\n",
    "where x is a vector of size 784 and delta a vector of size 10\n",
    " \n",
    " \n",
    "for $j \\neq t^{(n)}$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\delta_j^q x_i\n",
    "= - x_i q_j  \\cdot \\sum_k \\frac{1}{q_k}\n",
    "$\n",
    "\n",
    "In vector notation, using the required delta:\n",
    "\n",
    "$\n",
    "\\nabla_{\\bW_{:j}} \\mathcal{L}^{(n)} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial \\bW_{:j}}\n",
    "= \\mathbf{\\delta}^q\n",
    "\\frac{\\partial \\log q_j}{\\partial \\bW_{:j}}\n",
    "= \\textbf{x} {\\delta^q}^T  \n",
    "$\n",
    "\n",
    "\n",
    "-------------------\n",
    " -->\n",
    "\n",
    "$\n",
    "M = 28 \\cdot 28 = 784\n",
    "$\n",
    "\n",
    "$\n",
    "K = 10\n",
    "$\n",
    "\n",
    "$\n",
    "L = $# hidden layers (= 20 for the next assignment)$\n",
    "$\n",
    "\n",
    "$\n",
    "\\bX \\in \\mathbb{R}^{N \\times M}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bX_{n:} \\in \\mathbb{R}^{1 \\times M}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bV \\in \\mathbb{R}^{M \\times L}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bV_{m:} \\in \\mathbb{R}^{1 \\times L}\n",
    "$\n",
    "\n",
    "$\n",
    "\\ba \\in \\mathbb{R}^{L \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bh \\in \\mathbb{R}^{L \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bW \\in \\mathbb{R}^{L \\times K}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bW_{:k} \\in \\mathbb{R}^{L \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bW_{l:} \\in \\mathbb{R}^{1 \\times K}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bb \\in \\mathbb{R}^{K \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\log q_j = \\bW_{:j}^T \\bh + b_j \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "$\n",
    "\\log \\bq = \\bW^T \\bh + \\bb \\in \\mathbb{R}^{K \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "h_j = \\sigma(\\bV_{j:}^T \\bX_{n:}^T + a_j) \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "$\n",
    "\\bh = \\sigma(\\bV^T \\bX_{n:}^T + \\ba) \\in \\mathbb{R}^{L \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^q_j = \\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j} = t^{(n)}_j - \\frac{q_j}{Z} \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^{\\bq} = \\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log \\bq} = \\mathbb{I}(j = t^{(n)}) - \\frac{1}{Z} \\bq \\in \\mathbb{R}^{K \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^h_j = \\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial h_j} = \\delta^q \\cdot \\dfrac{\\partial \\log \\bq}{\\partial h_j} = \\delta^q \\cdot \\bW_{j:}^T \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^{\\bh} = \\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\bh} = \\delta^{\\bq} \\cdot \\dfrac{\\partial \\log \\bq}{\\partial \\bh} = \\bW \\cdot \\delta^q \\in \\mathbb{R}^{L \\times 1}\n",
    "$\n",
    "\n",
    "<!-- \n",
    "$\n",
    "\\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\bW} = \\delta^q_j \\cdot \\dfrac{\\partial \\log q}{\\partial \\bW} = \\delta^q_j \\cdot h_j\n",
    "$\n",
    " -->\n",
    " \n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial W_{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log q_j}\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\mathbf{\\delta}_j^q\n",
    "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
    "= \\delta_j^q h_i\n",
    "% = - h_i q_j  \\cdot \\sum_k \\frac{1}{q_k}\n",
    " \\in \\mathbb{R}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\bW} =\n",
    "\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial \\log \\bq}\n",
    "\\frac{\\partial \\log \\bq}{\\partial W}\n",
    "= \\mathbf{\\delta}^q\n",
    "\\frac{\\partial \\log q}{\\partial W}\n",
    "= \\bh \\cdot \\mathbf{\\delta}^{q^T}\n",
    "% = - \\bh^T \\bq \\cdot \\sum_k \\frac{1}{q_k}\n",
    " \\in \\mathbb{R}^{L \\times K}\n",
    "$\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\bb} = \\mathbf{\\delta}^q \\cdot \\dfrac{\\partial \\log \\bq}{\\partial \\bb} = \\delta^\\bq \\cdot 1 = \\delta^\\bq \\in \\mathbb{R}^{K \\times 1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\bV} = \\delta^\\bh \\cdot \\dfrac{\\partial \\bh}{\\partial \\bv} = ((\\delta^{\\bh} \\times (\\bh \\times (\\mathbf{1} - \\bh))) \\cdot \\bX_{n:})^T \\in \\mathbb{R}^{M \\times L}\n",
    "$\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial \\mathcal{L}^{(n)}}{\\partial \\ba} = \\delta^\\bh \\cdot \\dfrac{\\partial \\bh}{\\partial \\ba} = \\delta^{\\bh} \\times (\\bh \\times (\\mathbf{1} - \\bh)) \\in \\mathbb{R}^{L \\times 1}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cfcfec5959134f7f3fca2ba585a94fba",
     "grade": false,
     "grade_id": "cell-0bff945081e993fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.2 MAP optimization (10 points)\n",
    "\n",
    "You derived equations for finding the _maximum likelihood_ solution of the parameters. Explain, in a few sentences, how you could extend this approach so that it optimizes towards a _maximum a posteriori_ (MAP) solution of the parameters, with a Gaussian prior on the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "49d6376929b1cdf60a9ca9282512f1b4",
     "grade": true,
     "grade_id": "cell-1daef2744c010b73",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "For the maximum likelihood solution, we set the gradient of the log likelihood to zero.\n",
    "In the case of maximum a posteriori, we would first calculate the posterior using bayes rule, i.e. by multiplying the likelihood with the prior and dividing through the evidence, taking the log, and then to go through a similar derivation as done above to set the derivative of the posterior w.r.t. all the parameters to 0. The evidence term will be marginalized out and the entire derivation is as straightforward as the steps above.\n",
    "\n",
    "Given that you have already derived the above steps, you could simply add the gradient of the Gaussian w.r.t. the parameter to the gradient as derived above, which will give you the gradient of the MAP! This way, you can use SGD to find the MAP solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7e27334475d494b296af3afdf6bfcb4",
     "grade": false,
     "grade_id": "cell-2e56d8a567e2fb08",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.3. Implement and train a MLP (15 points)\n",
    "\n",
    "Implement an MLP model with a single hidden layer of **20 neurons**. \n",
    "Train the model for **10 epochs**.\n",
    "Test your implementation for learning rates of 1e-2, 1e-3 and 1e-4 and plot (in one graph) the conditional log-probability of the trainingset and validation set. \n",
    "\n",
    "For the best model plot the weights of the first layer for in epoch 0,4 and 9. \n",
    "\n",
    "\n",
    "- 10 points: Working MLP that learns with plots\n",
    "- +5 points: Fast, numerically stable, vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bca5a8a86e2d27a1c43d84fd9105421d",
     "grade": true,
     "grade_id": "cell-5d1924ace9e216e2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write all helper functions here\n",
    "from scipy.special import logsumexp, expit\n",
    "\n",
    "M = 28*28\n",
    "K = 10\n",
    "L = 20\n",
    "\n",
    "# 1.1.2 Compute gradient of log p(t|x;w,b) wrt w and b\n",
    "def mlp_logreg_gradient(x, t, v, w, a, b):\n",
    "    a = a.reshape((L, 1))\n",
    "    b = b.reshape((K, 1))\n",
    "    v = v.reshape((M, L))\n",
    "    w = w.reshape((L, K))\n",
    "    h = expit(np.add(v.T.dot(x.T), a))\n",
    "    logq = w.T.dot(h) + b\n",
    "    q = np.exp(logq)\n",
    "    logZ = logsumexp(logq)\n",
    "    Z = np.exp(logZ)\n",
    "    logp = logq - logZ\n",
    "    dL_dq = -1/Z * q\n",
    "    delta_q = dL_dq\n",
    "    delta_q[t[0],:] += 1\n",
    "    dL_dh = w.dot(delta_q)\n",
    "    delta_h = dL_dh\n",
    "    dL_db = delta_q\n",
    "    dL_dW = h.dot(delta_q.T)\n",
    "    dL_da = delta_h * h * (1 - h)\n",
    "    dL_dV = dL_da.dot(x).T\n",
    "    return logp[t].squeeze(), dL_dV, dL_dW, dL_da.squeeze(), dL_db.squeeze()\n",
    "\n",
    "\n",
    "def mlp_iter(x_train, t_train, v, w, a, b, lr=10**-4, val=False):\n",
    "    arr = np.arange(0,len(x_train))\n",
    "    np.random.shuffle(arr)\n",
    "    logp_tracker = 0\n",
    "\n",
    "    for i in arr:\n",
    "      logp_train, grad_v, grad_w, grad_a, grad_b = mlp_logreg_gradient(x_train[i:i+1, :], t_train[i:i+1], v, w, a, b)\n",
    "      logp_tracker += logp_train\n",
    "\n",
    "      if not val:\n",
    "        v = v + lr * grad_v\n",
    "        w = w + lr * grad_w\n",
    "        a = a + lr * grad_a\n",
    "        b = b + lr * grad_b\n",
    "    return (logp_tracker/len(arr)), v, w, a, b\n",
    "\n",
    "\n",
    "def test_mlp(x_train, t_train, x_valid, t_valid, v, w, a, b, lr=10**-4):\n",
    "    epochs = 10\n",
    "    y_train = []\n",
    "    y_val = []\n",
    "    v_epoch = []\n",
    "\n",
    "    # train the model for 10 epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "      logp_train, v, w, a, b = mlp_iter(x_train, t_train, v, w, a, b, lr)\n",
    "      logp_val, v_val, w_val, a_val, b_val = mlp_iter(x_valid, t_valid, v, w, a, b, lr, val=True)\n",
    "      y_train.append(logp_train)\n",
    "      y_val.append(logp_val)\n",
    "      v_epoch.append(v)\n",
    "\n",
    "    xs = range(epochs)\n",
    "    return (v, w, a, b, xs, y_train, y_val, v_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c73a8145fa846d154551bd6d85908790",
     "grade": true,
     "grade_id": "cell-94b75f65d3038a67",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden tests for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f3be36f9cf960203b49603b3719b8a4d",
     "grade": true,
     "grade_id": "cell-e9b2125a5ea8a22c",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write training code here:\n",
    "\n",
    "np.random.seed(1243)\n",
    "# single hidden layer of 20 neurons\n",
    "lrs = [1e-2, 1e-3, 1e-4]\n",
    "results = []\n",
    "# test your implementation for learning rates of 1e-2, 1e-3 and 1e-4\n",
    "for idx, lr in enumerate(lrs):\n",
    "  v = np.random.normal(size=(M,L), scale=0.1)\n",
    "  a = np.random.normal(size=L, scale=0.1)\n",
    "  w = np.random.normal(size=(L,K), scale=0.1)\n",
    "  b = np.random.normal(size=K, scale=0.1)\n",
    "  v, w, a, b, xs, y_train, y_val, v_epoch = test_mlp(x_train, t_train, x_valid, t_valid, v, w, a, b, lr)\n",
    "  results.append((v, w, a, b, xs, y_train, y_val, v_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9e9550447ee575c602a21489ce8534bb",
     "grade": true,
     "grade_id": "cell-b90dafbb9f41c1ed",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the train and validation logp for all three learning rates in one figure\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "for idx, res in enumerate(results):\n",
    "    v, w, a, b, xs, y_train, y_val, v_epoch = res\n",
    "    xs = np.array(xs) + 1\n",
    "    color = colors[idx]\n",
    "    \n",
    "    plt.plot(xs, y_train, f'{color}-o', linewidth=0.4, label=\"LogP for training data, lr={}\".format(lrs[idx]))\n",
    "    plt.plot(xs, y_val, f'{color}--o', label=\"LogP for validation data, lr={}\".format(lrs[idx]))\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Conditional Log Probability\")\n",
    "    plt.title(\"Plot of performance over several epochs for learning rate = {}\\nThe LogP for Training data displays a running average, \\nwhile the LogP for the Validation Data displays the average LogP with updated weights after the x'th epoch  \".format(lr))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()\n",
    "    \n",
    "  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "046552f90febc455e5c56d6bb68b8849",
     "grade": false,
     "grade_id": "cell-5b926040b792b57a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.1. Explain the learning curves (5 points)\n",
    "In less than 80 words, explain the observed behaviour for the different learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ca72ec2f983a4d7ca6083fffbf5f6063",
     "grade": true,
     "grade_id": "cell-8858cbe0e4dd02c7",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Low lr: The model requires more rounds to converge, but does not seem to overfit the model (logp(validation) > logp(training))\n",
    "\n",
    "Medium lr: It takes a few epochs to level out, but it quickly reaches a high level of log probability.\n",
    "\n",
    "High lr: It converges very quickly, but is overfitting the model on the training data (logp(training) > logp(validation)). Its performance on the validation data is still better than all other learning rates: you gain speed and don't lose model stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a03d026530585bc63f3cad6534b75f1b",
     "grade": false,
     "grade_id": "cell-6ae8cb5a4c246b97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.2. Explain the weights (5 points)\n",
    "In less than 80 words, explain how and why the weights of the hidden layer of the MLP differ from the logistic regression model, and relate this to the stronger performance of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f24e3bf890299503af4561915a23915",
     "grade": true,
     "grade_id": "cell-69b3830258566c6d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot the weights of the first layer.\n",
    "# Plot the weights of the first layer for the best model \n",
    "# For the best model plot the weights of the first layer for in epoch 0, 4 and 9.\n",
    "\n",
    "%pylab inline #Need to rerun these 2 lines, as otherwise the plots are too small (corrupted with size)\n",
    "plt.rcParams[\"figure.figsize\"] = [9,5]\n",
    "\n",
    "best_idx = 0  # learning rate 0.01 had the best score, though slight overfit as well\n",
    "for epoch in [0, 4, 9]:\n",
    "  print('epoch', epoch)\n",
    "  v, w, a, b, xs, y_train, y_val, v_epoch = results[best_idx]\n",
    "  plot_digits_2(v_epoch[epoch].T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f6cbf48a398f8722f4d403b957b2075",
     "grade": true,
     "grade_id": "cell-c4fdc27b1aab6828",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "As the MLP no longer needs to 'directly' capture its entire model in its single weight matrix, it is now able to create a division of labor among its two distinct layers. Above one can see that there seem to be some 'structures' in the weights, numbers can't be recognized as before. This is because the 784 input data points per image are first mapped to 20 dimensions which does not involve a clear human intuitive meaning anymore. The weights that perform this 'unintuitive' mapping are the weights that are plotted above. This additional layer of non-linear abstraction is what allows it to create a more powerful final model, explaining its better performance vs Logistic Regression (which is in essence a 1-layer neural network with a hidden layer with a linear identity activation function and softmax activation function in the last layer).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dd6a4522e0601e36fd37c9586a76236e",
     "grade": false,
     "grade_id": "cell-d10e996556dd40e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.2. Different activation functions (10 points)\n",
    "In the task above we use a sigmoid as an activation function.\n",
    "Two other popular choices for activation functions are tanh and the rectified linear unit (ReLU). The ReLU is defined as:\n",
    "\n",
    "$$f(x) = \\max(0.,x)$$\n",
    "\n",
    "You already derived the derivative of the softmax function above. Here, write down the derivative for both the tanh and the ReLU function. Furthermore, for all three, plot the function and its derivative in a range $x\\in[-3,3]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d67f15590158b453265dcd76b39e6631",
     "grade": true,
     "grade_id": "cell-e049422b1f9ce35e",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Write down the derivative of ReLU and tanh w.r.t. their respective argument:\n",
    "\n",
    "${\\frac{d}{dx}}\\tanh(x) = 1-\\tanh^{2}(x)$\n",
    "\n",
    "$\n",
    "{\\frac{d}{dx}}\\operatorname{ReLU}(x) =\n",
    "\\begin{cases} \n",
    "0 & \\text{if  }  x < 0 \\\\\n",
    "1 & \\text{if  }  x > 0 \\\\\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Name two properties that you would like your activation function to have (one sentence each). Why are they important?\n",
    "\n",
    "The activation function should ideally:\n",
    "- be non-linear: required for the network to be able to model non-linear functions\n",
    "<!-- all have this -->\n",
    "<!-- - be monotonically increasing: a higher input should not result in a lower output -->\n",
    "<!-- all have this -->\n",
    "<!-- - have a derivative that is monotonically increasing -->\n",
    "<!-- favors ReLU -->\n",
    "- have an easily calculated derivative: helps for performance\n",
    "<!-- favors ReLU but it's better suited for regression -->\n",
    "<!-- - be continuously differentiable:  -->\n",
    "<!-- not ReLU -->\n",
    "<!-- - approximate the identity function near the origin: helps learn efficiently when its weights are initialized with small random values. -->\n",
    "<!-- tanh > ReLU > sigmoid -->\n",
    "<!-- - be scale equivariant -->\n",
    "<!-- favors ReLU -->\n",
    "<!-- - be symmetric about the origin: otherwise it will be more prone to saturation of the later layers, making training more difficult. -->\n",
    "<!-- favors tanh -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9121f55cf725651c75e033f7ca13ee92",
     "grade": true,
     "grade_id": "cell-ecd7fbb4f1ece014",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the function and the derivative for the activations sigmoid, tanh and ReLU.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "x = np.arange(-1, 1, 0.01);\n",
    "\n",
    "plt.plot(x, expit(x), label='logistic')\n",
    "plt.plot(x, np.tanh(x), '--', label='tanh')\n",
    "plt.plot(x, np.maximum(0, x), linewidth=\"1.5\", label='relu')\n",
    "plt.plot(x, 1 - np.tanh(x) ** 2, label='d/dx tanh')\n",
    "plt.plot(x, expit(x) * (1 - expit(x)), label='d/dx logistic')\n",
    "plt.plot(x, (x > 0) * 1, '--', label='d/dx relu')\n",
    "plt.title(\"Plot of several activation functions\")\n",
    "plt.xlabel(\"Input values 'x'\")\n",
    "plt.ylabel(\"Output values 'y' after going through the activation function\")\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "928719563cd5a64fe7fbbe05361a4f9e",
     "grade": true,
     "grade_id": "cell-b80eb6b6816d09f7",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Now that you plotted the activations and derivatives, which activation do you think is the best? Why would you choose this activation function? For your answer consider what you named as essential properties for an activation function above. Keep your answer short at no more then 3 sentences.\n",
    "\n",
    "Each of these functions is non-linear, but ReLU seems to have the derivative that is easiest to calculate (given it's defined for $x=0$). This seems to make ReLU a good choice for our hidden layer, although the one-hot output layer of our classification model will still need an activation function yielding a total of $1$ over all the outputs, like softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c45d88c24c3a46a7b1ce3d417bf56c96",
     "grade": false,
     "grade_id": "cell-995c4d580f198861",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Notebook ran in {:2.3} minutes.'.format((time.time()-start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
